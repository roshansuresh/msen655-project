{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import statements\n",
    "# For generating dummy training data\n",
    "from pyDOE2 import *\n",
    "import numpy as np\n",
    "# For Neural Network\n",
    "from keras.models import load_model\n",
    "# For Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# For gaussian process regressors\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from math import sin, cos, pi, radians\n",
    "# For Optimization\n",
    "import copy\n",
    "from scipy.stats import norm\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "- This notebook details the implementation of a modified version of the multifidelity constrained Bayesian Optimization algorithm from: Ghoreishi, Seyede Fatemeh, and Douglas Allaire. \"Multi-information source constrained Bayesian optimization.\" Structural and Multidisciplinary Optimization 59, no. 3 (2019): 977-991.\n",
    "- The algorithm is first tested on a benchmark problem taken from the above mentioned paper to make sure the algorithm is working satisfactorily\n",
    "- After validation using the test problem, the algorithm is used on the composite plate problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function Definitions for Composite Plate problem\n",
    "- The model functions for the composite plate problem are defined below\n",
    "- The design parameters are fibre radius (r_f), fibre_angle (theta_f) and matrix Young's Modulus (E_m)\n",
    "- The objective to maximize is the buckling load (P_cr)\n",
    "- Three models are available: a high fidelity FEA model (using Abaqus Python Scipting), a low fidelity analytical model and a low fidelity Neural Network (trained using FEA model evaluations)\n",
    "- The constraint is that the volume fraction (v_f) must be less than 0.75.\n",
    "- Two simplified versions of this constraint are available, termed g_1(x) and g_2(x) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model functions for Composite Plate problem\n",
    "## Abaqus FEA model (Ground Truth)\n",
    "def abaqus_fea(x, job_num):\n",
    "    r_f = x[0] # in mm\n",
    "    theta_f = x[1] # in degrees\n",
    "    E_m = x[2]*1000 # in N/mm^2\n",
    "    \n",
    "    sys_command = ['abaqus','cae','noGUI=abaqus_fea_macro.py','--',str(r_f),str(theta_f),str(E_m),str(job_num)]\n",
    "    process = Popen(sys_command, stdout=PIPE, stderr=PIPE, cwd=r'H:/Desktop/MSEN 655 - Material Design Studio/python code/Neural Net Training Dataset',shell=True)\n",
    "    process.wait()\n",
    "    stdout, stderr = process.communicate()\n",
    "    #print(stdout)\n",
    "    output_string = stdout.decode(\"utf-8\")\n",
    "    #print(output_string)\n",
    "    \n",
    "    Eigen_1_pos_start = output_string.find('One') + 6\n",
    "    Eigen_1_pos_end = output_string.find('Endone') - 1\n",
    "    Eigen_2_pos_start = output_string.find('Two') + 6\n",
    "    Eigen_2_pos_end= output_string.find('Endtwo') - 1\n",
    "    Eigen_3_pos_start = output_string.find('Three') + 8\n",
    "    Eigen_3_pos_end = output_string.find('Endthree') - 1\n",
    "    \n",
    "    buckling_load_e1 = output_string[Eigen_1_pos_start:Eigen_1_pos_end-1]\n",
    "    buckling_load_e2 = output_string[Eigen_2_pos_start:Eigen_2_pos_end-1]\n",
    "    buckling_load_e3 = output_string[Eigen_3_pos_start:Eigen_3_pos_end-1]\n",
    "    \n",
    "    P_cr = [buckling_load_e1, buckling_load_e2, buckling_load_e3]\n",
    "    process.kill()\n",
    "    return P_cr\n",
    "\n",
    "## Analytical Model (Low Fidelity Model 1)\n",
    "def analytical(x,n_eigen):\n",
    "    # Here l,w,t are the length, width and thickness of the plate and E_f is the fibre Young's Modulus\n",
    "    # These are defined as global variables\n",
    "    r_f = x[0] # in mm\n",
    "    theta_f = x[1] # in degrees\n",
    "    E_m = x[2] # in GPa\n",
    "    P_cr = []\n",
    "    poisson_ratio = 0.3\n",
    "    v_f = volume_fraction(l,w,t,x)\n",
    "    ## Rule of Mixtures\n",
    "    E_effective = (E_f*v_f + (1-v_f)*E_m)*1e3 # Converting to N/mm^2\n",
    "    D = E_effective*t**3/(12*(1-poisson_ratio**2)) # Flexural Rigidity\n",
    "    for i in range(n_eigen):\n",
    "        m = i+1\n",
    "        n = 1\n",
    "        P_cr.append(D*(pi*l/m)**2*((m/l)**2+(n/w)**2)**2)\n",
    "    return P_cr\n",
    "    \n",
    "# Volume Fraction\n",
    "def volume_fraction(length,width,thickness,x_design):\n",
    "    r_f = x_design[0]\n",
    "    theta_f = x_design[1]\n",
    "    v_total = length*width*thickness\n",
    "    l_fibre_shortest = length/(6*cos(radians(theta_f)))\n",
    "    l_fibre_med = 2*l_fibre_shortest\n",
    "    l_fibre_longest = 3*l_fibre_shortest\n",
    "    vol_fibres = pi*(r_f**2)*(2*l_fibre_shortest + 2*l_fibre_med + l_fibre_longest)\n",
    "    vol_total = length*width*thickness\n",
    "    vol_frac = vol_fibres/vol_total\n",
    "    return vol_frac\n",
    "    \n",
    "## Neural Network (Lower Fidelity Model 2)\n",
    "#network = load_model('path/to/Neural/Net/model.h5')\n",
    "def neural_net(x):\n",
    "    P_cr = network.predict(x)\n",
    "    return P_cr\n",
    "\n",
    "## High fidelity Constraint Model\n",
    "def constraint_hf(x, vf_limit):\n",
    "    vf_x = volume_fraction(l,w,t,x)\n",
    "    constraint = vf_x - vf_limit\n",
    "    return constraint\n",
    "\n",
    "## First Lower Fidelity Constraint Model\n",
    "def constraint_g1(x):\n",
    "    \n",
    "    return const_g1\n",
    "\n",
    "## Second Lower Fidelity Constraint Model\n",
    "def constraint_g2(x):\n",
    "    \n",
    "    return const_g2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function Definitions for Test problem\n",
    "- The model functions for the test problem are defined below\n",
    "- The design space is one-dimensional\n",
    "- The objective to be maximized\n",
    "- Three objective models are available: a high fidelity model and two low fidelity models \n",
    "- Three constraint models are available: a high fidelity model and two low fidelity models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model functions for Test problem\n",
    "## High fidelity objective model\n",
    "def gt_obj(x):\n",
    "    y_gt = -(1.4 - 3*x)*(sin(18*x))\n",
    "    return y_gt\n",
    "\n",
    "## Low fidelity objective model 1\n",
    "def low_fid_obj1(x):\n",
    "    y_f1 = -(1.6 - 3*x)*(sin(18*x))\n",
    "    return y_f1\n",
    "\n",
    "## Low fidelity objective model 2\n",
    "def low_fid_obj2(x):\n",
    "    y_f2 = -(1.8 - 3*x)*sin(18*x + 0.1)\n",
    "    return y_f2\n",
    "\n",
    "## High fidelity constraint model\n",
    "def constraint_hf_test(x):\n",
    "    y_c_gt = x**2 - 1.2\n",
    "    return y_c_gt\n",
    "\n",
    "## Low fidelity constraint model 1\n",
    "def constraint_lf1_test(x):\n",
    "    y_c_lf1 = (x - 0.001)**2 - 1.2\n",
    "    return y_c_lf1\n",
    "\n",
    "## Low fidelity constraint model 2\n",
    "def constraint_lf2_test(x):\n",
    "    y_c_lf2 = (x + 0.02)**2 - 1.2\n",
    "    return y_c_lf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "- The functions defined below are represent operations performed frequently in the optimization algorithm\n",
    "- The Utility functions are defined in a generalised manner so as to apply to both the test problem and the composite plate problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some useful functions for the algorithm\n",
    "## GPR training given design set and response\n",
    "def gp_train(x,y,n_x,x_max,rand_state):\n",
    "    x_dim = len(x_max)\n",
    "    len_scale = []\n",
    "    for ia in range(x_dim):\n",
    "        len_scale.append(x_max[ia]*2/n_x)\n",
    "    #r_f_max = x_max[0]\n",
    "    #theta_f_max = x_max[1]\n",
    "    #E_m_max = x_max[2]\n",
    "    #len_scale = [r_f_max*2/n_x, theta_f_max*2/n_x, E_m_max*2/n_x]\n",
    "    kernel = Matern(length_scale=len_scale, nu=2.5)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, random_state=rand_state).fit(x,y)\n",
    "    return gpr\n",
    "\n",
    "## Computing fidelity variance (model discrepancy)\n",
    "def fidelity_variance(x,gp_gt,gp1,gp2):\n",
    "    # The fidelity variance is computed over a dataset different from the GP training datasets\n",
    "    y_gt = gp_gt.predict(x)\n",
    "    y_lf1 = gp1.predict(x)\n",
    "    y_lf2 = gp2.predict(x)\n",
    "    fid_var1 = np.mean(np.subtract(y_gt, y_lf1))**2\n",
    "    fid_var2 = np.mean(np.subtract(y_gt, y_lf2))**2\n",
    "    fid_var_vec = [fid_var1, fid_var2]\n",
    "    return fid_var_vec\n",
    "\n",
    "## Winkler Fusion\n",
    "def Winkler_fusion(x, gp_1, gp_2, sigma_f1, sigma_f2, x_max, get_gp=False):\n",
    "    n_des = len(x)\n",
    "    \n",
    "    gp1_means = np.zeros(n_des)\n",
    "    gp1_stds = np.zeros(n_des)\n",
    "    gp2_means = np.zeros(n_des)\n",
    "    gp2_stds = np.zeros(n_des)\n",
    "    gp_means = np.zeros([n_des,2])\n",
    "    gp_vars = np.zeros([n_des,2])\n",
    "    \n",
    "    [gp1_means, gp1_stds] = gp_1.predict(x, return_std=True)\n",
    "    [gp2_means, gp2_stds] = gp_2.predict(x, return_std=True)\n",
    "    gp_means = np.column_stack((gp1_means, gp2_means))\n",
    "    \n",
    "    var_1 = gp1_stds**2 + sigma_f1\n",
    "    var_2 = gp2_stds**2 + sigma_f2\n",
    "    gp_vars = np.column_stack((var_1, var_2))\n",
    "    \n",
    "    #print(gp_means)\n",
    "    #print(sigma_f1)\n",
    "    #print(sigma_f2)\n",
    "    #print(gp_vars)\n",
    "    rho_tilde = np.zeros([n_des,2,2])\n",
    "    rho = np.zeros([n_des,2,2])\n",
    "    covar = np.zeros([n_des,2,2])\n",
    "    mean_winkler = np.zeros(n_des)\n",
    "    var_winkler = np.zeros(n_des)\n",
    "    #print(rho_tilde[0].shape)\n",
    "    \n",
    "    for i in range(n_des):\n",
    "        # Compute rho_tilde matrix for current design sample\n",
    "        rho_tilde_x = np.zeros([2,2])\n",
    "        mean_vec = gp_means[i,:]\n",
    "        var_vec = gp_vars[i,:]\n",
    "        for j1 in range(2):\n",
    "            for k1 in range(2):\n",
    "                mean_j = mean_vec[j1]\n",
    "                mean_k = mean_vec[k1]\n",
    "                var_j = var_vec[j1]\n",
    "                var_k = var_vec[k1]\n",
    "                rho_tilde_x[j1,k1] = np.sqrt(var_j)/(np.sqrt((mean_j - mean_k)**2 + var_j))\n",
    "                #rho_tilde[i,j1,k1] = np.sqrt(var_j)/(np.sqrt((mean_j - mean_k)**2 + var_j))\n",
    "    \n",
    "        # Compute correlation matrix for current design sample\n",
    "        #rho_tilde_x = rho_tilde[i,:,:]\n",
    "        rho_x = np.zeros([2,2])\n",
    "        #var_vec = gp_vars[i,:]\n",
    "        for j2 in range(2):\n",
    "            for k2 in range(2):\n",
    "                var_j = var_vec[j2]\n",
    "                var_k = var_vec[k2]\n",
    "                rho_x[j2,k2] = (var_k/(var_j + var_k))*rho_tilde_x[j2,k2] + (var_j/(var_j + var_k))*rho_tilde_x[k2,j2]\n",
    "                #rho[i,j2,k2] = (var_k/(var_j + var_k))*rho_tilde_x[j2,k2] + (var_j/(var_j + var_k))*rho_tilde_x[k2,j2]\n",
    "        \n",
    "        # Compute covariance matrix for current design sample\n",
    "        #rho_x = rho[i,:,:]\n",
    "        covar_x = np.zeros([2,2])                                             \n",
    "        #var_vec = gp_vars[i,:]                                             \n",
    "        for j3 in range(2):\n",
    "            for k3 in range(2):\n",
    "                std_j = np.sqrt(var_vec[j3])\n",
    "                std_k = np.sqrt(var_vec[k3])\n",
    "                covar_x[j3,k3] = rho_x[j3,k3]*std_j*std_k\n",
    "                #covar[i,j3,k3] = rho_x[j3,k3]*std_j*std_k\n",
    "        \n",
    "        # Compute Winkler Mean\n",
    "        #print(rho_x)\n",
    "        #print(covar_x)\n",
    "        e_vec = np.ones(2)\n",
    "        #mean_vec = gp_means[i,:]\n",
    "        #covar_x = covar[i,:,:]\n",
    "        mean_winkler[i] = (np.linalg.multi_dot([np.transpose(e_vec), np.linalg.inv(covar_x), mean_vec]))/(np.linalg.multi_dot([np.transpose(e_vec), np.linalg.inv(covar_x), e_vec]))\n",
    "        var_winkler[i] = 1/(np.linalg.multi_dot([np.transpose(e_vec), np.linalg.inv(covar_x), e_vec]))\n",
    "        rho_tilde[i,:,:] = rho_tilde_x\n",
    "        rho[i,:,:] = rho_x\n",
    "        covar[i,:,:] = covar_x\n",
    "        \n",
    "    # Train the fused GPR (if required)\n",
    "    if (get_gp):\n",
    "        gp_fused = gp_train(x,mean_winkler,n_des,x_max,n_des)\n",
    "    else:\n",
    "        gp_fused = None\n",
    "    \n",
    "    return mean_winkler, var_winkler, gp_fused\n",
    "\n",
    "## Extracting feasible design points using the fused constraint GP\n",
    "def filter_designs(x,gp_constraint_fused):\n",
    "    n_des = len(x)\n",
    "    const_mean = np.zeros(n_des)\n",
    "    const_std = np.zeros(n_des)\n",
    "    x_feas = []\n",
    "    [const_mean, const_std] = gp_constraint_fused.predict(x, return_std=True)\n",
    "    for i in range(n_des):\n",
    "        if (const_mean[i] + 3*const_std[i] <= 0):\n",
    "            x_feas.append(x[i,:])\n",
    "    \n",
    "    return np.array(x_feas)\n",
    "\n",
    "## Expected Improvement computation\n",
    "def exp_improv(fused_means, fused_vars, y_best_current):\n",
    "    n_des = len(fused_means)\n",
    "    EI = np.zeros(n_des)\n",
    "    for i in range(n_des):\n",
    "        mean_i = fused_means[i]\n",
    "        var_i = fused_vars[i]\n",
    "        if (var_i == 0):\n",
    "            EI[i] = 0\n",
    "        else:\n",
    "            pdf_val = norm.pdf(mean_i, y_best_current, var_i)\n",
    "            cdf_val = norm.pdf(mean_i, y_best_current, var_i)\n",
    "            EI[i] = (mean_i - y_best_current)*cdf_val + var_i*pdf_val\n",
    "    ei = np.amax(EI)\n",
    "    return ei\n",
    "\n",
    "## Information Gain computation\n",
    "def info_gain(fused_means_prior, fused_vars_prior, fused_means_post, fused_vars_post, x_dim):\n",
    "    n_des = len(fused_means_prior)\n",
    "    kl_d = np.zeros(n_des)\n",
    "    for i in range(n_des):\n",
    "        kl_d[i] = ((fused_vars_prior[i]/fused_vars_post[i]) + ((fused_means_post[i] - fused_means_prior[i])**2/fused_vars_post[i]) - x_dim + np.log(fused_vars_post[i]/fused_vars_prior[i]))/2\n",
    "    ig = np.sum(kl_d)/n_des\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Optimization Algorithm\n",
    "- The cells below show the implementation of the multifidelity source constrained Bayesian Optimization algorithm\n",
    "- The algorithm is implemented in function form for easy test runs using a mathematical benchmark problem taken from: Ghoreishi, Seyede Fatemeh, and Douglas Allaire. \"Multi-information source constrained Bayesian optimization.\" Structural and Multidisciplinary Optimization 59, no. 3 (2019): 977-991.\n",
    "- The Optimization function (called MFCBO for Multi Fidelity Constrained Bayesian Optimization) is defined in a generalised manner for easy deployment for the test problem and the composite plate problem. \n",
    "- The function is defined to accomodate different maximization problems with two low fidelity objective and constraint models. \n",
    "- Future improvements include further generalization of the function to incorporate different number of objective and constraint low fidelity models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCBO(n_xvar, x_range, n_gt, n_f, n_const_gt, n_const, n_alt, n_test, cost_f, cost_c, max_cost, vf_limit, problem):\n",
    "    n_fmod = len(n_f) # Number of lower fidelity objective models\n",
    "    n_cmod = len(n_const) # Number of lower fidelity constraint models\n",
    "    xmax = []\n",
    "    for i in range(n_xvar):\n",
    "        xmax.append(x_range[i][1])\n",
    "        \n",
    "    # Generated design samples for objectives and constraints\n",
    "    x_f = {}\n",
    "    y_f = {}\n",
    "    x_c = {}\n",
    "    y_c = {}\n",
    "    gps_c = {}\n",
    "    gps_f = {}\n",
    "    # Costs of evaluation\n",
    "    cost_f_gt = cost_f[0]\n",
    "    cost_f_lf1 = cost_f[1]\n",
    "    cost_f_lf2 = cost_f[2]\n",
    "    cost_c_gt = cost_c[0]\n",
    "    cost_c_lf1 = cost_c[1]\n",
    "    cost_c_lf2 = cost_c[2]\n",
    "    \n",
    "    ## Define a job number (as a requirement for the abaqus fea function and to keep a track of the number of ground truth evaluations)\n",
    "    job_num_gtup = 0 # Number of abaqus evaluations for updating the ground truth GP\n",
    "    \n",
    "    ## Projecting the designs from Latin Hypercube Sampling to the appropriate design space\n",
    "    def sample_project(x, x_range, n_xvar):\n",
    "        # x = [a_val, b_val, c_val]\n",
    "        n_samp = len(x)\n",
    "        \n",
    "        x_projected = np.zeros([n_samp, n_xvar])\n",
    "        for i in range(n_xvar):\n",
    "            x_i_lower = x_range[i][0]\n",
    "            x_i_upper = x_range[i][1]\n",
    "            #x_i_projected = x[:,i]*(x_i_upper - x_i_lower) + np.ones(n_samp)*x_i_lower\n",
    "            for j in range(n_samp):\n",
    "                x_projected[j,i] = x[j,i]*(x_i_upper - x_i_lower) + x_i_lower\n",
    "    \n",
    "        return x_projected\n",
    "\n",
    "    ######################### STEP 1 - Initialization #########################\n",
    "    ## Generate design samples for constraints\n",
    "    x_const = lhs(n_xvar, samples=n_const_gt, criterion='maximin')\n",
    "    x_const_proj = sample_project(x_const, x_range, n_xvar)\n",
    "    for i2 in range(n_cmod):\n",
    "        x_ci = lhs(n_xvar, samples=n_const[i2], criterion='maximin')\n",
    "        x_ci_proj = sample_project(x_ci, x_range, n_xvar)\n",
    "        x_c[str(i2)] = x_ci_proj\n",
    "\n",
    "    ## Generate design samples for the ground truth and lower fidelity model GPs\n",
    "    x_gt = lhs(n_xvar, samples=n_gt, criterion='maximin')\n",
    "    x_gt_proj = sample_project(x_gt, x_range, n_xvar)\n",
    "    for i3 in range(n_fmod):\n",
    "        x_fi = lhs(n_xvar, samples=n_f[i3], criterion='maximin')\n",
    "        x_fi_proj = sample_project(x_fi, x_range, n_xvar)\n",
    "        x_f[str(i3)] = x_fi_proj\n",
    "    \n",
    "    def init_iter(x_c_gt, y_c_gt_i, x_constraints, y_constraints_i, x_f_gt, y_f_gt_i, x_objectives, y_objectives_i, n_c_gt, n_constraints, n_f_gt, n_objectives, gp_hf_c, gp_hf_f, cost_current, gt_update, iter_num):\n",
    "        ## Compute reference constraint values for the training samples based on the problem\n",
    "        if (problem == 'composite_plate'):\n",
    "            for i4 in range(n_cmod):\n",
    "                x_c_current = x_constraints[str(i4)]\n",
    "                y_c_current = np.zeros(n_constraints[i4])\n",
    "                if (i4 == 0):\n",
    "                    # Reference constraint values for lower fidelity model 1 GP\n",
    "                    if (iter_num == 0): # for the first iteration, evaluate all designs \n",
    "                        for i5 in range(n_constraints[i4]):\n",
    "                            y_c_current[i5] = constraint_lf1(x_c_current[i5,:])\n",
    "                            cost_current += cost_c_lf1\n",
    "                    else: # for subsequent iterations, simply evaluate last added design and append\n",
    "                        y_c_updated = constraint_lf1(x_c_current[-1,:])\n",
    "                        cost_current += cost_c_lf1\n",
    "                        y_c_i = y_constraints[str(i4)]\n",
    "                        y_c_current = y_c_i.append(y_c_updated)\n",
    "                elif (i4 == 1):\n",
    "                    # Reference constraint values for lower fidelity model 2 GP\n",
    "                    if (iter_num == 0):\n",
    "                        for i5 in range(n_constraints[i4]):\n",
    "                            y_c_current[i5] = constraint_lf2(x_c_current[i5,:])\n",
    "                            cost_current += cost_c_lf2\n",
    "                    else:\n",
    "                        y_c_updated = constraint_lf2(x_c_current[-1,:])\n",
    "                        cost_current += cost_c_lf1\n",
    "                        y_c_i = y_constraints[str(i4)]\n",
    "                        y_c_current = y_c_i.append(y_c_updated)\n",
    "                y_c[str(i4)] = y_c_current      \n",
    "        elif (problem == 'test'):\n",
    "            for i4 in range(n_cmod):\n",
    "                x_c_current = x_constraints[str(i4)]\n",
    "                y_c_current = np.zeros(n_constraints[i4])\n",
    "                if (i4 == 0):\n",
    "                    # Reference constraint values for lower fidelity model 1 GP\n",
    "                    if (iter_num == 0):\n",
    "                        for i5 in range(n_constraints[i4]):\n",
    "                            y_c_current[i5] = constraint_lf1_test(x_c_current[i5,:])\n",
    "                            cost_current += cost_c_lf1\n",
    "                    else:\n",
    "                        y_c_updated = constraint_lf1_test(x_c_current[-1,:])\n",
    "                        cost_current += cost_c_lf1\n",
    "                        y_c_i = y_constraints[str(i4)]\n",
    "                        y_c_current = y_c_i.append(y_c_updated)\n",
    "                elif (i4 == 1):\n",
    "                    # Reference objectives for lower fidelity model 2 GP\n",
    "                    if (iter_num == 0):\n",
    "                        for i5 in range(n_constraints[i4]):\n",
    "                            y_c_current[i5] = constraint_lf2_test(x_c_current[i5,:])\n",
    "                            cost_current += cost_c_lf2\n",
    "                    else:\n",
    "                        y_c_updated = constraint_lf2_test(x_c_current[-1,:])\n",
    "                        cost_current += cost_c_lf1\n",
    "                        y_c_i = y_constraints[str(i4)]\n",
    "                        y_c_current = y_c_i.append(y_c_updated)\n",
    "                y_c[str(i4)] = y_c_current\n",
    "            \n",
    "        #print(y_c)\n",
    "        ## Train the constraint GPs \n",
    "        for i6 in range(n_cmod):\n",
    "            x_c_current = x_constraints[str(i6)]\n",
    "            y_c_current = y_c[str(i6)]\n",
    "            gp_lfi_c = gp_train(x_c_current, y_c_current, n_constraints[i6], xmax, i6)\n",
    "            gps_c[str(i6)] = gp_lfi_c\n",
    "            \n",
    "        ## Updating Constraint Ground Truth GP (if required)\n",
    "        if (gt_update):\n",
    "            if (problem == 'composite_plate'):\n",
    "                # Reference constraint values for ground truth GP\n",
    "                if (iter_num == 0):\n",
    "                    y_const = np.zeros(n_c_gt)\n",
    "                    for i7 in range(n_c_gt):\n",
    "                        y_const[i7] = constraint_hf(x_c_gt[i7,:], vf_limit)\n",
    "                        cost_current += cost_c_gt\n",
    "                else:\n",
    "                    y_const_updated = constraint_hf(x_c_gt[-1,:], vf_limit)\n",
    "                    cost_current += cost_c_gt\n",
    "                    y_const = y_c_gt_i.append(y_const_updated)\n",
    "            elif (problem == 'test'):\n",
    "                # Reference constraint values for ground truth GP\n",
    "                if (iter_num == 0):\n",
    "                    y_const = np.zeros(n_c_gt)\n",
    "                    for i7 in range(n_c_gt):\n",
    "                        y_const[i7] = constraint_hf_test(x_c_gt[i7,:])\n",
    "                        cost_current += cost_c_gt\n",
    "                else:\n",
    "                    y_const_updated = constraint_hf_test(x_c_gt[-1,:], vf_limit)\n",
    "                    cost_current += cost_c_gt\n",
    "                    y_const = y_c_gt_i.append(y_const_updated)\n",
    "            # Train the new ground truth constraint GP\n",
    "            #print(x_const_proj)\n",
    "            #print(y_const)\n",
    "            gp_gt_c = gp_train(x_const_proj, y_const, n_c_gt, xmax, n_c_gt)\n",
    "        else:\n",
    "            gp_gt_c = gp_hf_c\n",
    "    \n",
    "        # Compute model discrepancy values for the two lower fidelity constraint models\n",
    "        x_fid = lhs(n_xvar, samples=n_c_gt, criterion='maximin')\n",
    "        x_fid_proj = sample_project(x_fid, x_range, n_xvar)\n",
    "        gp_lf1_c = gps_c['0']\n",
    "        gp_lf2_c = gps_c['1']\n",
    "        fid_var_lf_c = fidelity_variance(x_fid_proj, gp_gt_c, gp_lf1_c, gp_lf2_c)\n",
    "        # Fuse the lower fidelity constraint GPs\n",
    "        fid_var_lf1_c = fid_var_lf_c[0]\n",
    "        fid_var_lf2_c = fid_var_lf_c[1]\n",
    "        ## Filter the designs using the fused constraint GP\n",
    "        #gp_fused_c = Winkler_fusion(x_fid_proj, gp_lf1_c, gp_lf2_c, fid_var_lf1_c, fid_var_lf2_c, xmax)\n",
    "        #x_feas_gt = filter_designs(x_f_gt, gp_fused_c)\n",
    "        #n_feas_gt = len(x_feas_gt)\n",
    "        #n_feas = np.zeros(n_fmod)\n",
    "        #for i in range(n_fmod):\n",
    "            #x_f_i = x_objectives[str(i)]\n",
    "            #x_feas_i = filter_designs(x_f_i, gp_fused_c)\n",
    "            #x_feas[str(i)] = x_feas_i\n",
    "            #n_feas[i] = len(x_feas_i)\n",
    "    \n",
    "        ## Generate reference objectives for the feasible training samples based on the problem\n",
    "        if (problem == 'composite_plate'):\n",
    "            for i8 in range(n_fmod):\n",
    "                x_f_current = x_f[str(i8)]\n",
    "                y_f_current = np.zeros(n_f[i8])\n",
    "                if (i8 == 0):\n",
    "                    # Reference objectives for analytical model GP\n",
    "                    if (iter_num == 0):\n",
    "                        y_f1_3eig = np.zeros(3)\n",
    "                        for i9 in range(n_f[i8]):\n",
    "                            y_f1_3eig = analytical(x_f_current[i9,:], 3)\n",
    "                            y_f_current[i9] = y_f1_3eig[0] + 0.5*y_f1_3eig[1] + 0.25*y_f1_3eig[2]\n",
    "                            cost_current += cost_f_lf1\n",
    "                    else:\n",
    "                        y_f_updated = analytical(x_f_current[-1,:], 3)\n",
    "                        y_f_upd = y_f_updated[0] + 0.5*y_f_updated[1] + 0.25*y_f_updated[2]\n",
    "                        y_f_i = y_objectives_i[str(i8)]\n",
    "                        y_f_current = y_f_i.append(y_f_upd)\n",
    "                        cost_current += cost_f_lf1\n",
    "                elif (i8 == 1):\n",
    "                    # Reference objectives for neural net GP\n",
    "                    if (iter_num == 0):\n",
    "                        y_f2_3eig = np.zeros(3)\n",
    "                        for i9 in range(n_f[i8]):\n",
    "                            y_f2_3eig = neural_net(x_f_current[i9,:])\n",
    "                            y_f_current[i9] = y_f2_3eig[0] + 0.5*y_f2_3eig[1] + 0.25*y_f2_3eig[2]\n",
    "                            cost_current += cost_f_lf2\n",
    "                    else:\n",
    "                        y_f_updated = neural_net(x_f_current[-1,:])\n",
    "                        y_f_upd = y_f_updated[0] + 0.5*y_f_updated[1] + 0.25*y_f_updated[2]\n",
    "                        y_f_i = y_objectives_i[str(i8)]\n",
    "                        y_f_current = y_f_i.append(y_f_upd)\n",
    "                        cost_current += cost_f_lf1\n",
    "                y_f[str(i8)] = y_f_current \n",
    "        elif (problem == 'test'):\n",
    "            for i8 in range(n_fmod):\n",
    "                x_f_current = x_f[str(i8)]\n",
    "                y_f_current = np.zeros(n_f[i8])\n",
    "                if (i8 == 0):\n",
    "                    # Reference objectives for lower fidelity model 1 GP\n",
    "                    if (iter_num == 0):\n",
    "                        for i9 in range(n_f[i8]):\n",
    "                            y_f_current[i9] = low_fid_obj1(x_f_current[i9,:])\n",
    "                            cost_current += cost_f_lf1\n",
    "                    else:\n",
    "                        y_f_updated = low_fid_obj1(x_f_current[-1,:])\n",
    "                        cost_current += cost_f_lf1\n",
    "                        y_f_i = y_objectives_i[str(i8)]\n",
    "                        y_f_current = y_f_i.append(y_f_updated)\n",
    "                elif (i8 == 1):\n",
    "                    # Reference objectives for lower fidelity model 2 GP\n",
    "                    if (iter_num == 0):\n",
    "                        for i9 in range(n_f[i8]):\n",
    "                            y_f_current[i9] = low_fid_obj2(x_f_current[i9,:])\n",
    "                            cost_current += cost_f_lf2\n",
    "                    else:\n",
    "                        y_f_updated = low_fid_obj2(x_f_current[-1,:])\n",
    "                        cost_current += cost_f_lf1\n",
    "                        y_f_i = y_objectives_i[str(i8)]\n",
    "                        y_f_current = y_f_i.append(y_f_updated)\n",
    "                y_f[str(i8)] = y_f_current\n",
    "        \n",
    "        ## Train the objective GPs \n",
    "        for i10 in range(n_fmod):\n",
    "            x_f_current = x_f[str(i10)]\n",
    "            y_f_current = y_f[str(i10)]\n",
    "            gp_lfi_f = gp_train(x_f_current, y_f_current, n_f[i10], xmax, i10)\n",
    "            gps_f[str(i10)] = gp_lfi_f\n",
    "            \n",
    "        ## Updating Objective Ground Truth GP (if required)\n",
    "        if (gt_update):\n",
    "            if (problem == 'composite_plate'):\n",
    "                # Reference objectives for ground truth GP\n",
    "                if (iter_num == 0):\n",
    "                    y_gt_3eig = np.zeros(3)\n",
    "                    y_gt = np.zeros(n_gt)\n",
    "                    for i11 in range(n_gt):\n",
    "                        y_gt_3eig = abaqus_fea(x_gt[i11,:], job_num_gtup)\n",
    "                        y_gt[i11] = y_gt_3eig[0] + 0.5*y_gt_3eig[1] + 0.25*y_gt_3eig[2]\n",
    "                        job_num_gtup += 1\n",
    "                        cost_current += cost_f_gt\n",
    "                else:\n",
    "                    y_gt_3eig_updated = abaqus_fea(x_gt[-1,:], job_num_gtup)\n",
    "                    y_gt_upd = y_gt_3eig_updated[0] + 0.5*y_gt_3eig_updated[1] + 0.25*y_gt_3eig_updated[2]\n",
    "                    job_num_gt_up += 1\n",
    "                    cost_current += cost_f_gt\n",
    "                    y_gt = y_f_gt_i.append(y_gt_up)\n",
    "            elif (problem =='test'):\n",
    "                # Reference objectives for ground truth GP\n",
    "                if (iter_num == 0):\n",
    "                    y_gt = np.zeros(n_gt)\n",
    "                    for i11 in range(n_gt):\n",
    "                        y_gt[i11] = gt_obj(x_gt[i11,:])\n",
    "                        cost_current += cost_f_gt\n",
    "                else:\n",
    "                    y_gt_upd = gt_obj(x_gt[-1,:])\n",
    "                    cost_current += cost_f_gt\n",
    "                    y_gt = y_f_gt_i.append(y_gt_upd)\n",
    "            # Train the new objective ground truth GP\n",
    "            gp_gt_f = gp_train(x_gt, y_gt, n_gt, xmax, n_gt)\n",
    "        else:\n",
    "            gp_gt_f = gp_hf_f\n",
    "        \n",
    "        ## Compute model discrepancies for objective lower fidelity models\n",
    "        gp_lf1_f = gps_f['0']\n",
    "        gp_lf2_f = gps_f['1']\n",
    "        fid_var_lf_f = fidelity_variance(x_fid_proj, gp_gt_f, gp_lf1_f, gp_lf2_f)\n",
    "        \n",
    "        cost_up = copy.copy(cost_current)\n",
    "        \n",
    "        return gps_f, gps_c, x_f, y_f, n_f, x_constraints, y_c, n_constraints, fid_var_lf_f, fid_var_lf_c, cost_up\n",
    "\n",
    "    ######################### STEP 2 - Optimization Loop #########################\n",
    "    ## Initialize the evaluation cost and number of iterations\n",
    "    cost = 0\n",
    "    n_iter = 0\n",
    "    ## Define number of design samples for best design evaluation\n",
    "    n_best_samp = 100*n_xvar\n",
    "    ## For first iteration\n",
    "    gpr_hf_c = None\n",
    "    gpr_hf_f = None\n",
    "    hf_update = True\n",
    "    y_f_best_mean = 0\n",
    "    y_c_gt_init = np.zeros(n_const_gt)\n",
    "    y_f_gt_init = np.zeros(n_gt)\n",
    "    y_const_init = {}\n",
    "    y_f_init = {}\n",
    "    x_best_iter = {}\n",
    "    y_best_iter = []\n",
    "    y_best_std_iter = []\n",
    "    cost_iter = []\n",
    "    constraint_best_iter = []\n",
    "    constraint_best_std_iter = []\n",
    "    for i in range(n_cmod):\n",
    "        y_const_init[str(i)] = np.zeros(n_const[i])\n",
    "    for i in range(n_fmod):\n",
    "        y_f_init[str(i)] = np.zeros(n_f[i])\n",
    "    \n",
    "    while(cost <= max_cost):\n",
    "        print('Starting iteration ' + str(n_iter+1))\n",
    "        ## Initiailize iteration\n",
    "        gps_obj, gps_const, x_obj, y_obj, n_obj, x_const, y_const, n_const, mod_disc_f, mod_disc_c, cost_iter = init_iter(x_const_proj, y_c_gt_init, x_c, y_const_init, x_gt, y_f_gt_init, x_f, y_f_init, n_const_gt, n_const, n_gt, n_f, gpr_hf_c, gpr_hf_f, cost, hf_update, n_iter)\n",
    "        print('Initialization complete')\n",
    "        ## Generate x_alt, x_test and x_best_samp\n",
    "        x_alt_1 = lhs(n_xvar, samples=n_alt, criterion='maximin')\n",
    "        x_alt = sample_project(x_alt_1, x_range, n_xvar)\n",
    "        x_test_1 = lhs(n_xvar, samples=n_test, criterion='maximin')\n",
    "        x_test = sample_project(x_test_1, x_range, n_xvar)\n",
    "        x_best_samp_1 = lhs(n_xvar, samples=n_best_samp, criterion='maximin')\n",
    "        x_best_samp = sample_project(x_best_samp_1, x_range, n_xvar)\n",
    "        ## Filter the infeasible designs\n",
    "        gpr_c1 = gps_const['0']\n",
    "        gpr_c2 = gps_const['1']\n",
    "        sig_fid_c1 = mod_disc_c[0]\n",
    "        sig_fid_c2 = mod_disc_c[1]\n",
    "        mean_best_c_samp, var_best_c_samp, gp_fused_c = Winkler_fusion(x_test, gpr_c1, gpr_c2, sig_fid_c1, sig_fid_c2, xmax, get_gp=True)\n",
    "        x_alt_feas = filter_designs(x_alt, gp_fused_c)\n",
    "        n_alt_feas = len(x_alt_feas)\n",
    "        x_test_feas = filter_designs(x_test, gp_fused_c)\n",
    "        n_test_feas = len(x_test_feas)\n",
    "        print('Infeasible designs filtered')\n",
    "        \n",
    "        ## Objective optimization\n",
    "        EI_full = np.zeros([n_alt_feas, n_fmod])\n",
    "        for i in range(n_alt_feas):\n",
    "            ei_alt = np.zeros(n_fmod)\n",
    "            for j in range(n_fmod):\n",
    "                x_alt_current = x_alt_feas[i,:]\n",
    "                gp_f_j = gps_obj[str(j)]\n",
    "                n_samples_alt_i = 10\n",
    "                #y_samp_alt = np.zeros(n_samples_alt_i) \n",
    "                #[mean_gpi, std_gpi] = gp_f_i.predict(x_alt_current, return_std=True)\n",
    "                y_samp_alt = gp_f_j.sample_y(x_alt_current.reshape(1,-1), n_samples=n_samples_alt_i, random_state=1)\n",
    "                #print(y_samp_alt)\n",
    "                ei_samp = np.zeros(n_samples_alt_i)\n",
    "                for k in range(n_samples_alt_i):\n",
    "                    temp_gp_f_fused = None\n",
    "                    x_f_j = x_obj[str(j)]\n",
    "                    y_f_j = list(y_obj[str(j)])\n",
    "                    #print(y_f_j)\n",
    "                    x_f_j_new = np.vstack((x_f_j, x_alt_current))\n",
    "                    y_f_j.append(y_samp_alt[0][k])\n",
    "                    #print(y_f_j)\n",
    "                    temp_gp_f_j = gp_train(x_f_j_new, y_f_j, n_obj[j]+1, xmax, k)\n",
    "                    fid_var_f_j = mod_disc_f[j]\n",
    "                    if (j+1 > 1):\n",
    "                        fid_var_f_2 = mod_disc_f[j-1]\n",
    "                        gp_f_2 = gps_obj[str(j-1)]\n",
    "                        mean_gp_f_fused, var_gp_f_fused, temp_gp_f_fused = Winkler_fusion(x_test_feas, gp_f_2, temp_gp_f_j, fid_var_f_2, fid_var_f_j, xmax)\n",
    "                    else:\n",
    "                        fid_var_f_2 = mod_disc_f[j+1]\n",
    "                        gp_f_2 = gps_obj[str(j+1)]\n",
    "                        mean_gp_f_fused, var_gp_f_fused, temp_gp_f_fused = Winkler_fusion(x_test_feas, temp_gp_f_j, gp_f_2, fid_var_f_j, fid_var_f_2, xmax)\n",
    "                    ei_samp[k] = exp_improv(mean_gp_f_fused, var_gp_f_fused, y_f_best_mean)\n",
    "                ei_alt[j] = np.mean(ei_samp)\n",
    "            EI_full[i,:] = ei_alt\n",
    "        Ei_f_max = np.zeros(n_fmod)\n",
    "        Ei_f_argmax = np.zeros(n_fmod)\n",
    "        for j2 in range(n_fmod):\n",
    "            Ei_f = EI_full[:,j2]\n",
    "            cost_f_j = cost_f[j2+1]\n",
    "            utility = Ei_f/cost_f_j\n",
    "            Ei_f_max[j2] = np.amax(utility)\n",
    "            Ei_f_argmax[j2] = np.argmax(utility)\n",
    "        is_query_f = np.argmax(Ei_f_max)\n",
    "        x_query_index = Ei_f_argmax[is_query_f]\n",
    "        x_query_f = x_alt_feas[int(x_query_index),:]\n",
    "        print('Objective acquisition function maximization complete')\n",
    "        \n",
    "        ## Constraint Optimization\n",
    "        Ig_full = np.zeros([n_alt_feas, n_cmod])\n",
    "        for i in range(n_alt_feas):\n",
    "            ig_alt = np.zeros(n_cmod)\n",
    "            for j in range(n_cmod):\n",
    "                temp_gp_c_fused_prior = None\n",
    "                x_alt_current = x_alt_feas[i,:]\n",
    "                gp_c_j = gps_const[str(j)]\n",
    "                fid_var_c_j = mod_disc_c[j]\n",
    "                if (j+1 > 1):\n",
    "                    fid_var_c_2 = mod_disc_c[j-1]\n",
    "                    gp_c_2 = gps_const[str(j-1)]\n",
    "                    mean_prior_c_fused, var_prior_c_fused, temp_gp_c_fused_prior = Winkler_fusion(x_test_feas, gp_c_2, temp_gp_c_j, fid_var_c_2, fid_var_c_j, xmax)\n",
    "                else:\n",
    "                    fid_var_c_2 = mod_disc_c[j+1]\n",
    "                    gp_c_2 = gps_const[str(j+1)]\n",
    "                    mean_prior_c_fused, var_prior_c_fused, temp_gp_c_fused_prior = Winkler_fusion(x_test_feas, temp_gp_c_j, gp_c_2, fid_var_c_j, fid_var_c_2, xmax)\n",
    "                n_samples_alt_i = 10\n",
    "                #y_samp_alt = np.zeros(n_samples_alt_i) \n",
    "                #[mean_gpi, std_gpi] = gp_f_i.predict(x_alt_current, return_std=True)\n",
    "                y_samp_alt_c = gp_c_j.sample_y(x_alt_current.reshape(1,-1), n_samples=n_samples_alt_i, random_state=1)\n",
    "                ig_samp = np.zeros(n_samples_alt_i)\n",
    "                for k in range(n_samples_alt_i):\n",
    "                    temp_gp_c_fused = None\n",
    "                    x_c_j = x_const[str(j)]\n",
    "                    y_c_j = list(y_const[str(j)])\n",
    "                    x_c_j_new = np.vstack((x_c_j, x_alt_current))\n",
    "                    y_c_j.append(y_samp_alt_c[0][k])\n",
    "                    temp_gp_c_j = gp_train(x_c_j_new, y_c_j, n_const[j]+1, xmax, k)\n",
    "                    fid_var_c_j = mod_disc_c[j]\n",
    "                    if (j+1 > 1):\n",
    "                        fid_var_c_2 = mod_disc_c[j-1]\n",
    "                        gp_c_2 = gps_const[str(j-1)]\n",
    "                        mean_post_c_fused, var_post_c_fused, temp_gp_c_fused = winkler_fusion(x_test_feas, gp_c_2, temp_gp_c_j, fid_var_c_2, fid_var_c_j, xmax)\n",
    "                    else:\n",
    "                        fid_var_c_2 = mod_disc_c[j+1]\n",
    "                        gp_c_2 = gps_const[str(j+1)]\n",
    "                        mean_post_c_fused, var_post_c_fused, temp_gp_c_fused = winkler_fusion(x_test_feas, temp_gp_c_j, gp_c_2, fid_var_c_j, fid_var_c_2, xmax)\n",
    "                    ig_samp[k] = info_gain(mean_prior_c_fused, var_prior_c_fused, mean_post_c_fused, var_post_c_fused, n_xvar)\n",
    "                ig_alt[j] = np.mean(ig_samp)\n",
    "            Ig_full[i,:] = ig_alt\n",
    "        Ig_c_max = np.zeros(n_cmod)\n",
    "        Ig_c_argmax = np.zeros(n_cmod)\n",
    "        for j2 in range(n_cmod):\n",
    "            Ig_c = Ig_full[:,j2]\n",
    "            cost_c_j = cost_c[j2+1]\n",
    "            utility = Ig_c/cost_c_j\n",
    "            Ig_c_max[j2] = np.amax(utility)\n",
    "            Ig_c_argmax[j2] = np.argmax(utility)\n",
    "        is_query_c = np.argmax(Ig_c_max)\n",
    "        x_query_c_index = Ig_c_argmax[is_query_c]\n",
    "        x_query_c = x_alt_feas[int(x_query_c_index),:]\n",
    "        print('Constraint acquisition function maximization complete')\n",
    "        \n",
    "        ##  Update the training datasets for the relevant GPs\n",
    "        # For Objective GP\n",
    "        x_f_is_query = x_obj[str(is_query_f)]\n",
    "        x_f_is_updated = list(x_f_is_query).append(x_query_f)\n",
    "        x_obj[str(is_query_f)] = copy.copy(x_f_updated)\n",
    "        y_f_init = copy.copy(y_obj)\n",
    "        print('Objective query point appended to training dataset for appropriate information source')\n",
    "        \n",
    "        # For Constraint GP\n",
    "        x_c_is_query = x_const_proj[str(is_query_c)]\n",
    "        x_c_is_updated = list(x_c_is_query).append(x_query_c)\n",
    "        x_const[str(is_query_c)] = copy.copy(x_c_updated)\n",
    "        y_const_init = copy.copy(y_const)\n",
    "        print('Constraint query point appended to training dataset for appropriate information source')\n",
    "        \n",
    "        ## Current best design \n",
    "        gpr_f1 = gps_obj['0']\n",
    "        gpr_f2 = gps_obj['1']\n",
    "        sig_fid_f1 = mod_disc_f[0]\n",
    "        sig_fid_f2 = mod_disc_f[1]\n",
    "        mean_best_samp, var_best_samp, gp_f_fuse = Winkler_fusion(x_test_feas, gpr_f1, gpr_f2, sig_fid_f1, sig_fid_f2, xmax, get_gp=True)\n",
    "        [obj_test_mean, obj_test_std] = gp_f_fuse.predict(x_best_samp_feas, return_std=True)\n",
    "        obj_test_cond = obj_test_mean - 3*obj_test_std\n",
    "        x_f_pos = np.argmax(obj_test_cond)\n",
    "        x_f_best = x_test_feas[x_f_pos,:]\n",
    "        y_f_best_mean = obj_test_mean[x_f_pos]\n",
    "        y_f_best_std = obj_test_std[x_f_pos]\n",
    "        print('Best design point for current iteration determined')\n",
    "        \n",
    "        ## Evaluate estimated constraint value and uncertainty of best design using fused constraint GP\n",
    "        gpr_c1 = gps_const['0']\n",
    "        gpr_c2 = gps_const['1']\n",
    "        sig_fid_c1 = mod_disc_c[0]\n",
    "        sig_fid_c2 = mod_disc_c[1]\n",
    "        mean_best_c_samp, var_best_c_samp, gp_c_fuse = Winkler_fusion(x_test_feas, gpr_c1, gpr_c2, sig_fid_c1, sig_fid_c2, xmax, get_gp=True)\n",
    "        [const_best_mean, const_best_std] = gp_c_fuse.predict(x_f_best, return_std=True)\n",
    "        constraint_best_iter.append(const_best_mean)\n",
    "        constraint_best_std_iter.append(const_best_std)\n",
    "        print('Constraint for best design point for current iteration estimated')\n",
    "        \n",
    "        ## For Ground Truth GPs (if required)\n",
    "        if (n_iter%10 == 0):\n",
    "            # For Objective\n",
    "            x_gt_updated = list(x_gt).append(x_f_best)\n",
    "            x_gt = copy.copy(x_gt_updated)\n",
    "            \n",
    "            # For Constraint\n",
    "            #gpr_c1 = gps_const['0']\n",
    "            #gpr_c2 = gps_const['1']\n",
    "            #sig_fid_c1 = mod_disc_c[0]\n",
    "            #sig_fid_c2 = mod_disc_c[1]\n",
    "            #gp_c_fuse = Winkler_fusion(x_test_feas, gpr_c1, gpr_c2, sig_fid_c1, sig_fid_c2, xmax)\n",
    "            #[const_test_mean, const_test_std] = gp_c_fuse.predict(x_test_feas, return_std=True)\n",
    "            #const_test_cond = const_test_mean - 3*const_test_std\n",
    "            #x_c_pos = np.argmax(const_test_cond)\n",
    "            #x_c_add = x_test_feas[x_c_pos,:]\n",
    "            x_const_proj_updated = list(x_const_proj).append(x_f_best)\n",
    "            x_const_proj = copy.copy(x_const_proj_updated)\n",
    "            \n",
    "            hf_update = True\n",
    "            print('Ground Truth GPs will be updated in the next iteration')\n",
    "        else:\n",
    "            hf_update = False\n",
    "            \n",
    "        ## Increase iteration number and update cost\n",
    "        cost = copy.copy(cost_new)\n",
    "        x_best_iter[str(n_iter)] = x_f_best\n",
    "        y_best_iter.append(y_f_best_mean)\n",
    "        y_best_std_iter.append(y_f_best_std)\n",
    "        cost_iter.append(cost_new)\n",
    "        n_iter += 1\n",
    "    \n",
    "    x_best = x_f_best\n",
    "    y_best_mean = y_f_best_mean\n",
    "    y_best_std = y_f_best_std\n",
    "    print('Optimization concluded after ' + str(n_iter) + ' iterations')\n",
    "    \n",
    "    return x_best_iter, y_best_iter, y_best_std_iter, constraint_best_iter, constraint_best_std_iter, cost_iter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST PROBLEM\n",
    "nx_dim = 1\n",
    "x_range = [[0, 1.2]] # in units\n",
    "\n",
    "n_gt = 10 # number of design samples used to train the ground truth GP\n",
    "n_f1 = 40 # number of design samples used to train the analytical model GP\n",
    "n_f2 = 40 # number of design smaples used to train the neural network GP\n",
    "n_fs = [n_f1, n_f2]\n",
    "\n",
    "n_c_gt = 10 # number of design samples used to train the constraint ground truth GP\n",
    "n_const1 = 40 # number of design smaples used to train the first lower fidelity constraint GP\n",
    "n_const2 = 40 # number of design smaples used to train the second lower fidelity constraint GP\n",
    "n_cs = [n_const1, n_const2]\n",
    "\n",
    "n_test = 50 # number of design samples used to fuse GPs\n",
    "n_alt = 30 # number of candidate designs to sample in each iteration\n",
    "\n",
    "cost_gt_f = 1 # cost of evaluating a design using high fidelity model\n",
    "cost_f1 = 0.5 # cost of evaluating a design using low fidelity model 1\n",
    "cost_f2 = 0.3 # cost of evaluating a design using low fidelity model 2\n",
    "cost_fs = [cost_gt_f, cost_f1, cost_f2]\n",
    "\n",
    "cost_gt_c = 1 # cost of evaluating constraint for a design using ground truth \n",
    "cost_c1 = 0.2 # cost of evaluating constraint for a design using lower fidelity model 1\n",
    "cost_c2 = 0.1 # cost of evaluating constraint for a design using lower fidelity model 2\n",
    "cost_cs = [cost_gt_c, cost_c1, cost_c2]\n",
    "\n",
    "max_cost = 400 # stopping criterion in seconds\n",
    "vf_lim = 0.1 # not useful here but provided as a required input to the optimization function\n",
    "prob = 'test'\n",
    "\n",
    "x_best_dict, y_best_vec, y_best_std_vec, c_best_vec, c_best_std_vec, cost_vec = MFCBO(nx_dim, x_range, n_gt, n_fs, n_c_gt, n_cs, n_alt, n_test, cost_fs, cost_cs, max_cost, vf_lim, prob)\n",
    "\n",
    "## Plot the Best Function Values by iteration number\n",
    "n_iterations = len(y_best_vec)\n",
    "x_iter = np.linspace(1, n_iterations, n_iterations)\n",
    "fig1 = plt.figure()\n",
    "plt.plot(x_iter, y_best_vec, 'b-')\n",
    "plt.plot(x_iter, y_best_vec + 3*y_best_std_vec, 'r-')\n",
    "plt.plot(x_iter, y_best_vec - 3*y_best_std_vec, 'r-')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Best Function Value in units')\n",
    "plt.show()\n",
    "\n",
    "## Plot the Constraint Values of the best design by iteration number\n",
    "#n_iterations = len(y_best_vec)\n",
    "#x_iter = np.linspace(1, n_iterations, n_iterations)\n",
    "fig2 = plt.figure()\n",
    "plt.plot(x_iter, c_best_vec, 'b-')\n",
    "plt.plot(x_iter, c_best_vec + 3*c_best_std_vec, 'r-')\n",
    "plt.plot(x_iter, c_best_vec - 3*c_best_std_vec, 'r-')\n",
    "plt.xlabel('Iteration Number')\n",
    "plt.ylabel('Constraint Value of best design in units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPOSITE PLATE\n",
    "l = 3048 # length of composite plate in mm\n",
    "w = 3048 # width of composite plate in mm\n",
    "t = 254 # thickness of composite plate in mm\n",
    "E_f = 121 # Young's Modulus of fibre in GPa (same as abaqus fea, this is the Young's Modulus of Copper)\n",
    "\n",
    "rf_range = [25, (t/2)-20] # in mm\n",
    "thetaf_range = [0, 90] # in degrees\n",
    "Em_range = [10, 800] # in GPa \n",
    "\n",
    "n_gt = 10 # number of design samples used to train the ground truth GP\n",
    "n_f1 = 50 # number of design samples used to train the analytical model GP\n",
    "n_f2 = 50 # number of design smaples used to train the neural network GP\n",
    "n_const1 = 50 # number of design smaples used to train the first lower fidelity constraint GP\n",
    "n_const2 = 50 # number of design smaples used to train the second lower fidelity constraint GP\n",
    "\n",
    "n_test = 150 # number of design samples used to fuse GPs\n",
    "n_alt = 100 # number of candidate designs to sample in each iteration\n",
    "\n",
    "cost_gt_f = 100 # cost of evaluating a design using abaqus fea model (change to actual analysis time)\n",
    "cost_f1 = 0.5 # cost of evaluating a design using analytical model (change to actual analysis time)\n",
    "cost_f2 = 0.3 # cost of evaluating a design using neural network (change to actual analysis time)\n",
    "\n",
    "cost_gt_c = 1 # cost of evaluating constraint for a design using ground truth \n",
    "cost_c1 = 0.2 # cost of evaluating constraint for a design using lower fidelity model 1\n",
    "cost_c2 = 0.1 # cost of evaluating constraint for a design using lower fidelity model 2\n",
    "\n",
    "max_cost = 2000 # stopping criterion in seconds\n",
    "vf_limit = 0.75 # constraint limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman2\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(X, y)\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "y_samples = []\n",
    "y_samples = gpr.sample_y(X[0].reshape(1,-1),n_samples=10)\n",
    "print(y_samples)\n",
    "x_samples = np.linspace(1,10,10)\n",
    "fig = plt.figure()\n",
    "plt.plot(x_samples, y_samples[0], 'b-')\n",
    "plt.plot(y[0], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 0, 3]\n",
    "print(a[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
